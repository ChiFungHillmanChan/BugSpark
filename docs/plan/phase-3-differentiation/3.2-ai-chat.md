# 3.2 AI Chat / Intelligent Search

## Priority: MEDIUM | Complexity: HIGH

Contentsquare has Sense Chat, LogRocket has Ask Galileo, Sentry has AI Copilot. Natural language querying of bug data is a major 2026 trend. BugSpark already has Anthropic Claude integration — extending it to a chat interface creates a powerful debugging assistant.

---

## Feature Scope

### MVP
- Chat interface in dashboard sidebar
- Natural language queries about bugs: "What are the most common crash bugs this week?"
- Query console logs: "Show me all reports with 401 errors"
- Query network logs: "Which reports have slow API responses over 3 seconds?"
- Summarize trends: "What's the trend for login-related bugs?"

### Future
- Cross-report root cause analysis: "Are these 5 bugs related?"
- Suggested fixes based on codebase patterns
- Proactive alerts: "I noticed a spike in API timeout errors"

## Architecture

### Approach: RAG (Retrieval Augmented Generation)

1. User asks a question
2. System converts question to a search query
3. Search retrieves relevant reports (using embeddings or full-text search)
4. Retrieved reports + question are sent to Claude
5. Claude generates an answer grounded in the data

### Option A: pgvector (Simpler, lower cost)

Use PostgreSQL with pgvector extension for embedding storage.

```sql
-- Add embedding column to reports
ALTER TABLE reports ADD COLUMN embedding vector(1536);
CREATE INDEX ON reports USING ivfflat (embedding vector_cosine_ops);
```

Generate embeddings using Anthropic's embedding API (or OpenAI's text-embedding-3-small for cost efficiency).

### Option B: Full-text search + Claude (Simplest MVP)

Skip embeddings entirely. Use PostgreSQL full-text search to find relevant reports, then let Claude analyze them.

```python
# Simple approach: use existing search + Claude
async def ai_chat(db: AsyncSession, project_id: str, question: str, user: User):
    # Step 1: Extract search terms from question
    search_terms = await extract_search_terms(question)

    # Step 2: Search reports
    reports = await search_reports(db, project_id, search_terms, limit=20)

    # Step 3: Format context
    context = format_reports_for_context(reports)

    # Step 4: Send to Claude
    response = await anthropic_client.messages.create(
        model="claude-sonnet-4-20250514",
        messages=[
            {"role": "user", "content": f"""You are a bug analysis assistant for the project.
            Here are the relevant bug reports:

            {context}

            User question: {question}

            Answer based only on the provided bug reports. If you can't find relevant information, say so."""}
        ]
    )
    return response.content[0].text
```

**Recommendation: Start with Option B** (simplest, no new infrastructure), then upgrade to Option A when data volume justifies it.

### API Endpoint

```python
# packages/api/app/routers/ai_chat.py

@router.post("/projects/{project_id}/ai/chat")
async def ai_chat_endpoint(
    project_id: str,
    request: AIChatRequest,
    db: AsyncSession = Depends(get_db_session),
    current_user: User = Depends(get_active_user),
):
    # Check plan access (Team+ only)
    if current_user.plan not in ("team", "enterprise") and current_user.role != "superadmin":
        raise ForbiddenException("AI Chat requires Team plan or above")

    # Verify project access
    await verify_project_member(db, project_id, current_user.id)

    response = await ai_chat_service.answer(db, project_id, request.question, current_user)
    return AIChatResponse(answer=response.answer, sources=response.source_report_ids)
```

### Chat Schema

```python
class AIChatRequest(CamelModel):
    question: str
    conversation_id: str | None = None  # For follow-up questions

class AIChatResponse(CamelModel):
    answer: str
    sources: list[str]  # Report IDs referenced in the answer
    conversation_id: str
```

### Dashboard UI

```typescript
// packages/dashboard/src/components/ai-chat/ai-chat-panel.tsx

// Slide-out panel or sidebar
// Chat message bubbles (user + AI)
// Source links (clickable report IDs)
// Suggested questions (quick prompts)
// Streaming response display
```

Suggested quick prompts:
- "What are the most critical unresolved bugs?"
- "Summarize bugs reported this week"
- "Are there any patterns in recent crash reports?"
- "Which bugs affect the login flow?"

### Conversation Memory

Store chat history per project per user:

```python
class AIChatMessage(Base):
    __tablename__ = "ai_chat_messages"

    id = Column(UUID, primary_key=True)
    conversation_id = Column(UUID, nullable=False, index=True)
    project_id = Column(UUID, ForeignKey("projects.id"))
    user_id = Column(UUID, ForeignKey("users.id"))
    role = Column(String(20))  # "user" or "assistant"
    content = Column(Text)
    source_report_ids = Column(JSON, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
```

## Implementation Steps

1. Create `ai_chat_service.py` with search + Claude approach
2. Create `ai_chat.py` router
3. Create chat request/response schemas
4. Create `AIChatMessage` model for conversation history
5. Create Alembic migration
6. Create dashboard chat panel component
7. Add streaming support (SSE)
8. Add suggested quick prompts
9. Add source linking (click report ID → navigate to report)
10. Add plan gating (Team+)
11. Rate limit: 20 chat messages per hour

## Cost Estimation

- Each chat query: ~3K input tokens (context) + 500 output tokens
- At Sonnet pricing: ~US$0.02 per query
- 100 queries/day = ~US$60/month
- Rate limit protects against runaway costs

## Tests to Add

- `test_ai_chat_returns_relevant_answer`
- `test_ai_chat_cites_source_reports`
- `test_ai_chat_team_plan_required`
- `test_ai_chat_rate_limited`
- `test_ai_chat_conversation_history_persisted`
- `test_ai_chat_project_scoped` (can't query other projects' bugs)
